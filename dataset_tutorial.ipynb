{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for slicing error message\n",
    "import sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First  5 pairs in x: \n",
      " [[0.28685276 0.63100043]\n",
      " [0.64307607 0.34817589]\n",
      " [0.72643868 0.86969373]\n",
      " [0.49805229 0.5265903 ]\n",
      " [0.48830834 0.3464211 ]] \n",
      "\n",
      "[0.28685276 0.63100043]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.sample((100,2))\n",
    "print(\"First  5 pairs in x: \\n\", x[:5], \"\\n\" )\n",
    "# make a dataset from a numpy array\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First  5 features in features: \n",
      " [[0.47763518 0.10118737]\n",
      " [0.88673148 0.1753077 ]\n",
      " [0.39861486 0.55617885]\n",
      " [0.18044922 0.29457104]\n",
      " [0.99059056 0.98068558]] \n",
      "\n",
      "First  5 labels in labels: \n",
      " [[0.17502057]\n",
      " [0.15836931]\n",
      " [0.2899051 ]\n",
      " [0.58924805]\n",
      " [0.39903524]] \n",
      "\n",
      "(array([0.47763518, 0.10118737]), array([0.17502057]))\n"
     ]
    }
   ],
   "source": [
    "# using two numpy arrays\n",
    "features, labels = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "print(\"First  5 features in features: \\n\", features[:5], \"\\n\" )\n",
    "print(\"First  5 labels in labels: \\n\", labels[:5], \"\\n\" )\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features,labels))\n",
    "\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset shape:  ()\n",
      "dataset:  <TensorSliceDataset shapes: (2,), types: tf.float32>\n",
      "[0.9658345 0.9390756]\n"
     ]
    }
   ],
   "source": [
    "# using a tensor\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tf.random_uniform([100, 2]))\n",
    "print(\"dataset shape: \", np.shape(dataset))\n",
    "print(\"dataset: \", (dataset))\n",
    "\n",
    "iter = dataset.make_initializable_iterator()\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iter.initializer)\n",
    "    print(sess.run(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First  5 pairs in data: \n",
      " [[0.98745784 0.76018421]\n",
      " [0.23984296 0.52829117]\n",
      " [0.36874844 0.58930026]\n",
      " [0.79183049 0.03393259]\n",
      " [0.69591143 0.67569711]] \n",
      "\n",
      "[0.9874578 0.7601842]\n"
     ]
    }
   ],
   "source": [
    "# using a placeholder\n",
    "x = tf.placeholder(tf.float32, shape=[None,2])\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "\n",
    "data = np.random.sample((100,2))\n",
    "print(\"First  5 pairs in data: \\n\", data[:5], \"\\n\" )\n",
    "\n",
    "iter = dataset.make_initializable_iterator()\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iter.initializer, feed_dict={ x: data })\n",
    "    print(sess.run(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]] \n",
      "\n",
      "[[2]\n",
      " [3]] \n",
      "\n",
      "[[3]\n",
      " [4]\n",
      " [5]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from generator\n",
    "sequence = np.array([[[1]],[[2],[3]],[[3],[4],[5]]])\n",
    "\n",
    "def generator():\n",
    "    for el in sequence:\n",
    "        yield el\n",
    "\n",
    "dataset = tf.data.Dataset().batch(1).from_generator(generator,\n",
    "                                           output_types= tf.int64, \n",
    "                                           output_shapes=(tf.TensorShape([None, 1])))\n",
    "\n",
    "iter = dataset.make_initializable_iterator()\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iter.initializer)\n",
    "    print(sess.run(el),\"\\n\")\n",
    "    print(sess.run(el),\"\\n\")\n",
    "    print(sess.run(el),\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1., 2.], dtype=float32), array([0.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# initializable iterator to switch between data\n",
    "EPOCHS = 10\n",
    "\n",
    "x, y = tf.placeholder(tf.float32, shape=[None,2]), tf.placeholder(tf.float32, shape=[None,1])\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "\n",
    "train_data = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "test_data = (np.array([[1,2]]), np.array([[0]]))\n",
    "\n",
    "iter = dataset.make_initializable_iterator()\n",
    "features, labels = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "#     initialise iterator with train data\n",
    "    sess.run(iter.initializer, feed_dict={ x: train_data[0], y: train_data[1]})\n",
    "    for _ in range(EPOCHS):\n",
    "        sess.run([features, labels])\n",
    "#     switch to test data\n",
    "    sess.run(iter.initializer, feed_dict={ x: test_data[0], y: test_data[1]})\n",
    "    print(sess.run([features, labels]))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Train:  [array([0.30806043, 0.1002669 ]), array([0.1682616])]\n",
      "Train:  [array([0.37143655, 0.8316497 ]), array([0.44658544])]\n",
      "Train:  [array([0.68813665, 0.09992314]), array([0.77531029])]\n",
      "Train:  [array([0.15658696, 0.50050271]), array([0.06538125])]\n",
      "Train:  [array([0.89562205, 0.4644704 ]), array([0.22641821])]\n",
      "Train:  [array([0.8849491 , 0.18134612]), array([0.18742334])]\n",
      "Train:  [array([0.72141324, 0.22196128]), array([0.74209331])]\n",
      "Train:  [array([0.53484265, 0.51675819]), array([0.27482371])]\n",
      "Train:  [array([0.61364882, 0.60404416]), array([0.02574554])]\n",
      "Train:  [array([0.54130389, 0.62880368]), array([0.840464])]\n",
      "\n",
      "Test:  [array([0.89432699, 0.51992212]), array([0.83652204])]\n",
      "Test:  [array([0.76349031, 0.31493326]), array([0.95570562])]\n",
      "Test:  [array([0.89225831, 0.48527156]), array([0.78004326])]\n",
      "Test:  [array([0.45998032, 0.66667351]), array([0.93703739])]\n",
      "Test:  [array([0.61365475, 0.45763402]), array([0.84526684])]\n",
      "Test:  [array([0.06362408, 0.49845725]), array([0.33785973])]\n",
      "Test:  [array([0.23547254, 0.64970082]), array([0.6617016])]\n",
      "Test:  [array([0.01605124, 0.37127647]), array([0.96830388])]\n",
      "Test:  [array([0.29025174, 0.82502863]), array([0.92200274])]\n",
      "Test:  [array([0.24292015, 0.3313715 ]), array([0.4544349])]\n",
      "Epoch:  1\n",
      "Train:  [array([0.30806043, 0.1002669 ]), array([0.1682616])]\n",
      "Train:  [array([0.37143655, 0.8316497 ]), array([0.44658544])]\n",
      "Train:  [array([0.68813665, 0.09992314]), array([0.77531029])]\n",
      "Train:  [array([0.15658696, 0.50050271]), array([0.06538125])]\n",
      "Train:  [array([0.89562205, 0.4644704 ]), array([0.22641821])]\n",
      "Train:  [array([0.8849491 , 0.18134612]), array([0.18742334])]\n",
      "Train:  [array([0.72141324, 0.22196128]), array([0.74209331])]\n",
      "Train:  [array([0.53484265, 0.51675819]), array([0.27482371])]\n",
      "Train:  [array([0.61364882, 0.60404416]), array([0.02574554])]\n",
      "Train:  [array([0.54130389, 0.62880368]), array([0.840464])]\n",
      "\n",
      "Test:  [array([0.89432699, 0.51992212]), array([0.83652204])]\n",
      "Test:  [array([0.76349031, 0.31493326]), array([0.95570562])]\n",
      "Test:  [array([0.89225831, 0.48527156]), array([0.78004326])]\n",
      "Test:  [array([0.45998032, 0.66667351]), array([0.93703739])]\n",
      "Test:  [array([0.61365475, 0.45763402]), array([0.84526684])]\n",
      "Test:  [array([0.06362408, 0.49845725]), array([0.33785973])]\n",
      "Test:  [array([0.23547254, 0.64970082]), array([0.6617016])]\n",
      "Test:  [array([0.01605124, 0.37127647]), array([0.96830388])]\n",
      "Test:  [array([0.29025174, 0.82502863]), array([0.92200274])]\n",
      "Test:  [array([0.24292015, 0.3313715 ]), array([0.4544349])]\n"
     ]
    }
   ],
   "source": [
    "# Reinitializable iterator to switch between Datasets\n",
    "#EVIDENCE: Can re-read the same data\n",
    "NO_OF_BATCHES = 10\n",
    "DATA_ITEMS = 10\n",
    "NO_OF_EPOCHS = 2\n",
    "# making fake data using numpy\n",
    "train_data = (np.random.sample((DATA_ITEMS,2)), np.random.sample((DATA_ITEMS,1)))\n",
    "test_data = (np.random.sample((10,2)), np.random.sample((10,1)))\n",
    "\n",
    "# create two datasets, one for training and one for test\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_data)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_data)\n",
    "\n",
    "# create a iterator of the correct shape and type\n",
    "iter = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "\n",
    "assert(train_dataset.output_types == test_dataset.output_types)\n",
    "assert(train_dataset.output_shapes == test_dataset.output_shapes)\n",
    "\n",
    "features, labels = iter.get_next()\n",
    "\n",
    "# create the initialisation operations\n",
    "train_init_op = iter.make_initializer(train_dataset)\n",
    "test_init_op = iter.make_initializer(test_dataset)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for epoch in range(NO_OF_EPOCHS):\n",
    "        print(\"Epoch: \", epoch)\n",
    "        sess.run(train_init_op) # switch to train dataset\n",
    "        for _ in range(NO_OF_BATCHES):\n",
    "            print(\"Train: \",sess.run([features, labels]))\n",
    "        print()\n",
    "        \n",
    "        COMMENT_OUT = True\n",
    "        if(not COMMENT_OUT):\n",
    "            sess.run(train_init_op) # switch to train dataset\n",
    "            for _ in range(NO_OF_BATCHES):\n",
    "                print(\"Train: \",sess.run([features, labels]))\n",
    "            print()\n",
    "            \n",
    "        sess.run(test_init_op) # switch to val dataset\n",
    "        for _ in range(NO_OF_BATCHES):\n",
    "            print(\"Test: \",sess.run([features, labels]))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.36652708 0.34349123] [0.97863287]\n",
      "[0.5297945  0.42868015] [0.78588873]\n",
      "[0.2527375  0.95566773] [0.29431063]\n",
      "[0.83061224 0.8994813 ] [0.76414174]\n",
      "[0.4356476  0.96186346] [0.46852523]\n",
      "[0.6881188  0.90323484] [0.75912625]\n",
      "[0.8467667  0.25164708] [0.68335015]\n",
      "[0.6074306  0.80885905] [0.6883512]\n",
      "[0.565559   0.92677295] [0.1901109]\n",
      "[0.09625694 0.09132044] [0.18951891]\n",
      "[0.6515981 0.889733 ] [0.8997203]\n"
     ]
    }
   ],
   "source": [
    "# feedable iterator to switch between iterators\n",
    "EPOCHS = 10\n",
    "# making fake data using numpy\n",
    "train_data = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "test_data = (np.random.sample((10,2)), np.random.sample((10,1)))\n",
    "# create placeholder\n",
    "x, y = tf.placeholder(tf.float32, shape=[None,2]), tf.placeholder(tf.float32, shape=[None,1])\n",
    "# create two datasets, one for training and one for test\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "# create the iterators from the dataset\n",
    "train_iterator = train_dataset.make_initializable_iterator()\n",
    "test_iterator = test_dataset.make_initializable_iterator()\n",
    "# same as in the doc https://www.tensorflow.org/programmers_guide/datasets#creating_an_iterator\n",
    "handle = tf.placeholder(tf.string, shape=[])\n",
    "iter = tf.data.Iterator.from_string_handle(\n",
    "    handle, train_dataset.output_types, train_dataset.output_shapes)\n",
    "next_elements = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    train_handle = sess.run(train_iterator.string_handle())\n",
    "    test_handle = sess.run(test_iterator.string_handle())\n",
    "    \n",
    "    # initialise iterators. In our case we could have used the 'one-shot' iterator instead,\n",
    "    # and directly feed the data insted the Dataset.from_tensor_slices function, but this\n",
    "    # approach is more general\n",
    "    sess.run(train_iterator.initializer, feed_dict={ x: train_data[0], y: train_data[1]})\n",
    "    sess.run(test_iterator.initializer, feed_dict={ x: test_data[0], y: test_data[1]})\n",
    "    \n",
    "    for _ in range(EPOCHS):\n",
    "        x,y = sess.run(next_elements, feed_dict = {handle: train_handle})\n",
    "        print(x, y)\n",
    "        \n",
    "    x,y = sess.run(next_elements, feed_dict = {handle: test_handle})\n",
    "    print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First  8 pairs in x: \n",
      " [[0.57516571 0.72312181]\n",
      " [0.3508486  0.40030373]\n",
      " [0.31443465 0.13794457]\n",
      " [0.06788421 0.79406055]\n",
      " [0.56184218 0.0654742 ]\n",
      " [0.27286945 0.46926317]\n",
      " [0.58854606 0.29394427]\n",
      " [0.16162587 0.73755893]] \n",
      "\n",
      "First  2 batches (each of 4 pairs) in dataset:\n",
      "[[0.57516571 0.72312181]\n",
      " [0.3508486  0.40030373]\n",
      " [0.31443465 0.13794457]\n",
      " [0.06788421 0.79406055]]\n",
      "[[0.56184218 0.0654742 ]\n",
      " [0.27286945 0.46926317]\n",
      " [0.58854606 0.29394427]\n",
      " [0.16162587 0.73755893]]\n"
     ]
    }
   ],
   "source": [
    "# BATCHING\n",
    "BATCH_SIZE = 4\n",
    "x = np.random.sample((100,2))\n",
    "print(\"First  8 pairs in x: \\n\", x[:8], \"\\n\" )\n",
    "\n",
    "# make a dataset from a numpy array\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x).batch(BATCH_SIZE)\n",
    "\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"First  2 batches (each of 4 pairs) in dataset:\")\n",
    "    print(sess.run(el));    print(sess.run(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPEAT\n",
    "BATCH_SIZE = 4\n",
    "x = np.array([[1],[2],[3],[4]])\n",
    "# make a dataset from a numpy array\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "dataset = dataset.repeat() #<<<<<========================\n",
    "                                                        #^\n",
    "iter = dataset.make_one_shot_iterator()                 #^\n",
    "el = iter.get_next()                                    #^\n",
    "                                                        #^\n",
    "#with tf.Session() as sess:                              #^\n",
    "#     this will run forever #==Because of ==========>>>>>>\n",
    "#    while True:\n",
    "#        print(sess.run(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of dataset before map():  <TensorSliceDataset shapes: (1,), types: tf.int64>\n",
      "Type of dataset after map():  <MapDataset shapes: (1,), types: tf.int64>\n",
      "i:  0 . x[i]:  [2]\n",
      "i:  0 , x[i]:  [4]\n",
      "i:  0 , x[i]:  [6]\n",
      "i:  1 . x[i]:  [8]\n",
      "i:  1 ; Exception thrown as iterator is at the end. \n",
      "\tException Type:  <class 'tensorflow.python.framework.errors_impl.OutOfRangeError'> ; \n",
      "\tIn File:  <ipython-input-13-b14a58f43f2b> ;  \n",
      "\tAt Line No:  19\n",
      "i:  2 ; Exception thrown as iterator is at the end. \n",
      "\tException Type:  <class 'tensorflow.python.framework.errors_impl.OutOfRangeError'> ; \n",
      "\tIn File:  <ipython-input-13-b14a58f43f2b> ;  \n",
      "\tAt Line No:  18\n",
      "i:  3 ; Exception thrown as iterator is at the end. \n",
      "\tException Type:  <class 'tensorflow.python.framework.errors_impl.OutOfRangeError'> ; \n",
      "\tIn File:  <ipython-input-13-b14a58f43f2b> ;  \n",
      "\tAt Line No:  18\n"
     ]
    }
   ],
   "source": [
    "# MAP\n",
    "x = np.array([[1],[2],[3],[4]])\n",
    "# make a dataset from a numpy array\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "print(\"Type of dataset before map(): \", dataset)\n",
    "\n",
    "dataset = dataset.map(lambda x: x*2)\n",
    "print(\"Type of dataset after map(): \", dataset)\n",
    "\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "#     this will run forever.  \n",
    "#RM: NOT correct. There is no dataset = dataset.repeat() here.\n",
    "        for i in range(len(x)):\n",
    "            try:\n",
    "                print(\"i: \", i, \". x[i]: \", sess.run(el))\n",
    "                print(\"i: \", i, \", x[i]: \", sess.run(el))\n",
    "                print(\"i: \", i, \", x[i]: \", sess.run(el))\n",
    "            except:\n",
    "                \"\"\"\n",
    "                from:\n",
    "                https://stackoverflow.com/questions/1278705/python-when-i-catch-an-exception-how-do-i-get-the-type-file-and-line-number\n",
    "                \"\"\"\n",
    "                exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "                fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "                print(\"i: \", i, \"; Exception thrown as iterator is at the end. \\n\\tException Type: \", exc_type,\\\n",
    "                      \"; \\n\\tIn File: \", fname, \"; \", \\\n",
    "                      \"\\n\\tAt Line No: \",exc_tb.tb_lineno)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [4]\n",
      " [3]\n",
      " [2]] \n",
      "\n",
      "Exception thrown as iterator is at the end. \n",
      "\tException Type:  <class 'tensorflow.python.framework.errors_impl.OutOfRangeError'> ; \n",
      "\tIn File:  <ipython-input-14-915945608dd8> ;  \n",
      "\tAt Line No:  15\n",
      "But With new session there should not be any problem.\n",
      " [[2]\n",
      " [1]\n",
      " [4]\n",
      " [3]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SHUFFLE\n",
    "BATCH_SIZE = 4\n",
    "x = np.array([[1],[2],[3],[4]])\n",
    "# make a dataset from a numpy array\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "dataset = dataset.shuffle(buffer_size=100)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        print(sess.run(el), \"\\n\")\n",
    "        print(sess.run(el), \"\\n\")\n",
    "    except:\n",
    "        \"\"\"\n",
    "        from:\n",
    "        https://stackoverflow.com/questions/1278705/python-when-i-catch-an-exception-how-do-i-get-the-type-file-and-line-number\n",
    "        \"\"\"\n",
    "        exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "        print(\"Exception thrown as iterator is at the end. \\n\\tException Type: \", exc_type,\\\n",
    "              \"; \\n\\tIn File: \", fname, \"; \", \\\n",
    "              \"\\n\\tAt Line No: \",exc_tb.tb_lineno)  \n",
    "        \n",
    "#With new session there should not be any problem        \n",
    "with tf.Session() as sess:\n",
    "    print(\"But With new session there should not be any problem.\\n\", sess.run(el), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Loss: 0.1149\n",
      "Iter: 1, Loss: 0.1109\n",
      "Iter: 2, Loss: 0.1075\n",
      "Iter: 3, Loss: 0.1045\n",
      "Iter: 4, Loss: 0.1021\n",
      "Iter: 5, Loss: 0.1001\n",
      "Iter: 6, Loss: 0.0985\n",
      "Iter: 7, Loss: 0.0973\n",
      "Iter: 8, Loss: 0.0965\n",
      "Iter: 9, Loss: 0.0959\n"
     ]
    }
   ],
   "source": [
    "# how to pass the value to a model\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "# using two numpy arrays\n",
    "features, labels = (np.array([np.random.sample((100,2))]), \n",
    "                    np.array([np.random.sample((100,1))]))\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features,labels)).repeat().batch(BATCH_SIZE)\n",
    "\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "x, y = iter.get_next()\n",
    "\n",
    "# make a simple model\n",
    "net = tf.layers.dense(x, 8, activation=tf.tanh) # pass the first value from iter.get_next() as input\n",
    "net = tf.layers.dense(net, 8, activation=tf.tanh)\n",
    "prediction = tf.layers.dense(net, 1, activation=tf.tanh)\n",
    "\n",
    "loss = tf.losses.mean_squared_error(prediction, y) # pass the second value from iter.get_net() as label\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(EPOCHS):\n",
    "        _, loss_value = sess.run([train_op, loss])\n",
    "        print(\"Iter: {}, Loss: {:.4f}\".format(i, loss_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[0.24807023, 0.17148587],\n",
      "       [0.86233073, 0.65456855],\n",
      "       [0.00860412, 0.78815776],\n",
      "       [0.46045172, 0.71933   ],\n",
      "       [0.09725407, 0.17776896],\n",
      "       [0.27247015, 0.5730647 ],\n",
      "       [0.9112755 , 0.96162033],\n",
      "       [0.4934017 , 0.9538497 ],\n",
      "       [0.75804895, 0.41793817],\n",
      "       [0.74809986, 0.826399  ]], dtype=float32), array([[0.32958052],\n",
      "       [0.6368118 ],\n",
      "       [0.02543208],\n",
      "       [0.56506634],\n",
      "       [0.00580711],\n",
      "       [0.0143747 ],\n",
      "       [0.75209737],\n",
      "       [0.44360676],\n",
      "       [0.49445188],\n",
      "       [0.492034  ]], dtype=float32))\n",
      "List Of features:  [[0.24807023 0.17148587]\n",
      " [0.86233073 0.65456855]\n",
      " [0.00860412 0.78815776]\n",
      " [0.46045172 0.71933   ]\n",
      " [0.09725407 0.17776896]\n",
      " [0.27247015 0.5730647 ]\n",
      " [0.9112755  0.96162033]\n",
      " [0.4934017  0.9538497 ]\n",
      " [0.75804895 0.41793817]\n",
      " [0.74809986 0.826399  ]]\n",
      "List of labels:  [[0.32958052]\n",
      " [0.6368118 ]\n",
      " [0.02543208]\n",
      " [0.56506634]\n",
      " [0.00580711]\n",
      " [0.0143747 ]\n",
      " [0.75209737]\n",
      " [0.44360676]\n",
      " [0.49445188]\n",
      " [0.492034  ]]\n",
      "Training...\n",
      "Epoch No: 0, Loss: 0.5849\n",
      "Epoch No: 1, Loss: 0.3401\n",
      "Epoch No: 2, Loss: 0.2131\n",
      "Epoch No: 3, Loss: 0.1441\n",
      "Epoch No: 4, Loss: 0.1174\n",
      "Epoch No: 5, Loss: 0.1057\n",
      "Epoch No: 6, Loss: 0.1005\n",
      "Epoch No: 7, Loss: 0.1017\n",
      "Epoch No: 8, Loss: 0.1041\n",
      "Epoch No: 9, Loss: 0.1027\n",
      "Test Loss: 0.069394\n",
      "Training Again...\n",
      "Epoch No: 0, Loss: 0.0924\n",
      "Epoch No: 1, Loss: 0.0922\n",
      "Epoch No: 2, Loss: 0.0921\n",
      "Epoch No: 3, Loss: 0.0919\n",
      "Epoch No: 4, Loss: 0.0917\n",
      "Epoch No: 5, Loss: 0.0915\n",
      "Epoch No: 6, Loss: 0.0913\n",
      "Epoch No: 7, Loss: 0.0911\n",
      "Epoch No: 8, Loss: 0.0909\n",
      "Epoch No: 9, Loss: 0.0907\n",
      "Test Loss: 0.068772\n",
      "Training yet Again...\n",
      "Epoch No: 0, Loss: 0.0905\n",
      "Epoch No: 1, Loss: 0.0903\n",
      "Epoch No: 2, Loss: 0.0902\n",
      "Epoch No: 3, Loss: 0.0900\n",
      "Epoch No: 4, Loss: 0.0898\n",
      "Epoch No: 5, Loss: 0.0897\n",
      "Epoch No: 6, Loss: 0.0895\n",
      "Epoch No: 7, Loss: 0.0893\n",
      "Epoch No: 8, Loss: 0.0892\n",
      "Epoch No: 9, Loss: 0.0890\n",
      "Test Loss: 0.068547\n",
      "Training yet Again...\n",
      "Epoch No: 0, Loss: 0.0889\n",
      "Epoch No: 1, Loss: 0.0887\n",
      "Epoch No: 2, Loss: 0.0886\n",
      "Epoch No: 3, Loss: 0.0885\n",
      "Epoch No: 4, Loss: 0.0883\n",
      "Epoch No: 5, Loss: 0.0882\n",
      "Epoch No: 6, Loss: 0.0881\n",
      "Epoch No: 7, Loss: 0.0880\n",
      "Epoch No: 8, Loss: 0.0879\n",
      "Epoch No: 9, Loss: 0.0878\n",
      "Test Loss: 0.068648\n"
     ]
    }
   ],
   "source": [
    "# Wrapping all together -> Switch between train and test set using Initializable iterator\n",
    "NO_OF_EPOCHS = 10\n",
    "######################\n",
    "#RM\n",
    "NoOfTrainingSamples = 100\n",
    "NoOfTestSamples=20\n",
    "INPUT_SHAPE = [None, 2] #Input data. 2-D Array of features\n",
    "OUTPUT_SHAPE = [None, 1] #Output Classes. 1-D array of Labels\n",
    "BATCH_SIZE = 10\n",
    "#######################\n",
    "# create a placeholder to dynamically switch between batch sizes\n",
    "batch_size = tf.placeholder(tf.int64)\n",
    "\n",
    "x, y = tf.placeholder(tf.float32, shape=INPUT_SHAPE), \\\n",
    "                tf.placeholder(tf.float32, shape=OUTPUT_SHAPE)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x, y)).batch(batch_size).repeat()\n",
    "\n",
    "iter = dataset.make_initializable_iterator()\n",
    "features, labels = iter.get_next()\n",
    "############################\n",
    "# make a simple model\n",
    "net = tf.layers.dense(features, 8, activation=tf.tanh) # pass the first value from iter.get_next() as input\n",
    "net = tf.layers.dense(net, 8, activation=tf.tanh)\n",
    "prediction = tf.layers.dense(net, 1, activation=tf.tanh)\n",
    "\n",
    "loss = tf.losses.mean_squared_error(prediction, labels) # pass the second value from iter.get_net() as label\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "No_Of_Batches = NoOfTrainingSamples//BATCH_SIZE\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # using two numpy arrays\n",
    "    train_data = (np.random.sample((NoOfTrainingSamples,INPUT_SHAPE[1])), \\\n",
    "                  np.random.sample((NoOfTrainingSamples,OUTPUT_SHAPE[1])))\n",
    "    test_data = (np.random.sample((NoOfTestSamples,INPUT_SHAPE[1])), \\\n",
    "                 np.random.sample((NoOfTestSamples,OUTPUT_SHAPE[1])))\n",
    "\n",
    "    # initialise iterator with train data\n",
    "    #See https://www.tensorflow.org/api_docs/python/tf/data/Dataset#make_initializable_iterator\n",
    "    # The initializer property returns the tf.operation that is run. It will initialize the iterator.\n",
    "    # See https://www.tensorflow.org/api_docs/python/tf/data/Iterator#initializer\n",
    "    sess.run(iter.initializer, feed_dict={ x: train_data[0], y: train_data[1], batch_size: BATCH_SIZE})\n",
    "    ListOfFeatureLabelPairs = sess.run(iter.get_next())\n",
    "    print(ListOfFeatureLabelPairs)\n",
    "    print(\"List Of features: \", ListOfFeatureLabelPairs[0])\n",
    "    print(\"List of labels: \", ListOfFeatureLabelPairs[1])\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # using two numpy arrays\n",
    "    train_data = (np.random.sample((NoOfTrainingSamples,INPUT_SHAPE[1])), \\\n",
    "                  np.random.sample((NoOfTrainingSamples,OUTPUT_SHAPE[1])))\n",
    "    test_data = (np.random.sample((NoOfTestSamples,INPUT_SHAPE[1])), \\\n",
    "                 np.random.sample((NoOfTestSamples,OUTPUT_SHAPE[1])))\n",
    "\n",
    "    # initialise iterator with train data\n",
    "    #See https://www.tensorflow.org/api_docs/python/tf/data/Dataset#make_initializable_iterator\n",
    "    # The initializer property returns the tf.operation that is run. It will initialize the iterator.\n",
    "    # See https://www.tensorflow.org/api_docs/python/tf/data/Iterator#initializer\n",
    "    sess.run(iter.initializer, feed_dict={ x: train_data[0], y: train_data[1], batch_size: BATCH_SIZE})\n",
    "    print('Training...')\n",
    "    for epoch in range(NO_OF_EPOCHS):\n",
    "        tot_loss = 0\n",
    "        for _ in range(No_Of_Batches + 1):\n",
    "            _, loss_value = sess.run([train_op, loss])\n",
    "            tot_loss += loss_value\n",
    "        print(\"Epoch No: {}, Loss: {:.4f}\".format(epoch, tot_loss / No_Of_Batches))\n",
    "        \n",
    "    # initialise iterator with test data\n",
    "    sess.run(iter.initializer, feed_dict={ x: test_data[0], y: test_data[1], batch_size: test_data[0].shape[0]})\n",
    "    print('Test Loss: {:4f}'.format(sess.run(loss)))\n",
    "\n",
    "    ##########################################################\n",
    "    # Repeat above to prove that the iterator does not stall when it comes to the end\n",
    "    # initialise iterator with train data\n",
    "    sess.run(iter.initializer, feed_dict={ x: train_data[0], y: train_data[1], batch_size: BATCH_SIZE})\n",
    "    print('Training Again...')\n",
    "    for epoch in range(NO_OF_EPOCHS):\n",
    "        tot_loss = 0\n",
    "        for _ in range(No_Of_Batches):\n",
    "            _, loss_value = sess.run([train_op, loss])\n",
    "            tot_loss += loss_value\n",
    "        print(\"Epoch No: {}, Loss: {:.4f}\".format(epoch, tot_loss / No_Of_Batches))\n",
    "        \n",
    "    # initialise iterator with test data\n",
    "    sess.run(iter.initializer, feed_dict={ x: test_data[0], y: test_data[1], batch_size: test_data[0].shape[0]})\n",
    "    print('Test Loss: {:4f}'.format(sess.run(loss)))\n",
    "\n",
    "    ##########################################################\n",
    "    # Repeat yet again to prove that the iterator does not stall when it comes to the end\n",
    "    # initialise iterator with train data\n",
    "    sess.run(iter.initializer, feed_dict={ x: train_data[0], y: train_data[1], batch_size: BATCH_SIZE})\n",
    "    print('Training yet Again...')\n",
    "    for epoch in range(NO_OF_EPOCHS):\n",
    "        tot_loss = 0\n",
    "        for _ in range(No_Of_Batches):\n",
    "            _, loss_value = sess.run([train_op, loss])\n",
    "            tot_loss += loss_value\n",
    "        print(\"Epoch No: {}, Loss: {:.4f}\".format(epoch, tot_loss / No_Of_Batches))\n",
    "        \n",
    "    # initialise iterator with test data\n",
    "    sess.run(iter.initializer, feed_dict={ x: test_data[0], y: test_data[1], batch_size: test_data[0].shape[0]})\n",
    "    print('Test Loss: {:4f}'.format(sess.run(loss)))\n",
    "\n",
    "    ##########################################################\n",
    "    # Repeat yet again to prove that the iterator does not stall when it comes to the end\n",
    "    # initialise iterator with train data\n",
    "    sess.run(iter.initializer, feed_dict={ x: train_data[0], y: train_data[1], batch_size: BATCH_SIZE})\n",
    "    print('Training yet Again...')\n",
    "    for epoch in range(NO_OF_EPOCHS):\n",
    "        tot_loss = 0\n",
    "        for _ in range(No_Of_Batches):\n",
    "            _, loss_value = sess.run([train_op, loss])\n",
    "            tot_loss += loss_value\n",
    "        print(\"Epoch No: {}, Loss: {:.4f}\".format(epoch, tot_loss / No_Of_Batches))\n",
    "        \n",
    "    # initialise iterator with test data\n",
    "    sess.run(iter.initializer, feed_dict={ x: test_data[0], y: test_data[1], batch_size: test_data[0].shape[0]})\n",
    "    print('Test Loss: {:4f}'.format(sess.run(loss)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping all together -> Switch between train and test set using Reinitializable iterator\n",
    "EPOCHS = 10\n",
    "######################\n",
    "#RM\n",
    "NoOfTrainingSamples = 100\n",
    "NoOfTestSamples=20\n",
    "#######################\n",
    "# create a placeholder to dynamically switch between batch sizes\n",
    "batch_size = tf.placeholder(tf.int64)\n",
    "\n",
    "x, y = tf.placeholder(tf.float32, shape=[None,2]), tf.placeholder(tf.float32, shape=[None,1])\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x,y)).batch(batch_size).repeat()\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x,y)).batch(batch_size) # always batch even \n",
    "                                                                #if you want to one shot it\n",
    "# using two numpy arrays\n",
    "train_data = (np.random.sample((NoOfTrainingSamples,2)), np.random.sample((NoOfTrainingSamples,1)))\n",
    "test_data = (np.random.sample((NoOfTestSamples,2)), np.random.sample((NoOfTestSamples,1)))\n",
    "\n",
    "# create a iterator of the correct shape and type\n",
    "iter = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "features, labels = iter.get_next()\n",
    "\n",
    "# create the initialisation operations\n",
    "train_init_op = iter.make_initializer(train_dataset)\n",
    "test_init_op = iter.make_initializer(test_dataset)\n",
    "\n",
    "# make a simple model\n",
    "net = tf.layers.dense(features, 8, activation=tf.tanh) # pass the first value from iter.get_next() as input\n",
    "net = tf.layers.dense(net, 8, activation=tf.tanh)\n",
    "prediction = tf.layers.dense(net, 1, activation=tf.tanh)\n",
    "\n",
    "loss = tf.losses.mean_squared_error(prediction, labels) # pass the second value from iter.get_net() as label\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "n_batches = int(NoOfTrainingSamples/BATCH_SIZE)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # initialise iterator with train data\n",
    "    sess.run(train_init_op, feed_dict = {x : train_data[0], y: train_data[1], batch_size: 16})\n",
    "    print('Training...')\n",
    "    for i in range(EPOCHS):\n",
    "        tot_loss = 0\n",
    "        for _ in range(n_batches):\n",
    "            _, loss_value = sess.run([train_op, loss])\n",
    "            tot_loss += loss_value\n",
    "        print(\"Iter: {}, Loss: {:.4f}\".format(i, tot_loss / n_batches))\n",
    "    # initialise iterator with test data\n",
    "    sess.run(test_init_op, feed_dict = {x : test_data[0], y: test_data[1], batch_size:len(test_data[0])})\n",
    "    print('Test Loss: {:4f}'.format(sess.run(loss)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a csv\n",
    "CSV_PATH = './tweets.csv'\n",
    "dataset = tf.contrib.data.make_csv_dataset(CSV_PATH, batch_size=32, shuffle=False) \n",
    "#RM: Original code default value of shuffle (True) was used. With shuffle set to False we read the CSV\n",
    "#file, row-by-row.\n",
    "\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "next = iter.get_next()\n",
    "#print(\"iter.get_next(): \", next, \"\\n\") # next is a dict with key=columns names and value=column data\n",
    "\n",
    "#################################################\n",
    "#\n",
    "inputs, labels = next['text'], next['sentiment']\n",
    "#################################################\n",
    "\n",
    "with  tf.Session() as sess:\n",
    "    print(sess.run([inputs,labels]), \"\\n\")\n",
    "    print(sess.run([inputs,labels]), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_time = {}\n",
    "# copied form https://medium.com/pythonhive/python-decorator-to-measure-the-execution-time-of-methods-fa04cb6bb36d\n",
    "def how_much(method):\n",
    "    def timed(*args, **kw):\n",
    "        ts = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        te = time.time()\n",
    "        \n",
    "        if 'log_time' in kw:\n",
    "            name = kw.get('log_name', method.__name__)\n",
    "            kw['log_time'][name] = (te - ts)\n",
    "            \n",
    "        return result\n",
    "    return timed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark\n",
    "import time\n",
    "DATA_SIZE = 5000\n",
    "DATA_SHAPE = ((32,32),(20,))\n",
    "BATCH_SIZE = 64 \n",
    "N_BATCHES = DATA_SIZE // BATCH_SIZE\n",
    "EPOCHS = 10\n",
    "\n",
    "test_size = (DATA_SIZE//100)*20 \n",
    "\n",
    "DUMMY = -1\n",
    "\n",
    "train_shape = (DATA_SHAPE)\n",
    "print(\"DATA_SHAPE:\",train_shape)\n",
    "train_shape = (DATA_SHAPE[0])\n",
    "print(\"DATA_SHAPE[0]:\",train_shape)\n",
    "train_shape = ((DUMMY, *DATA_SHAPE))\n",
    "print(\"DUMMY, *DATA_SHAPE:\",train_shape)\n",
    "train_shape = ((DUMMY, (*DATA_SHAPE))[0])\n",
    "print(\"DUMMY, (*DATA_SHAPE))[0]:\",train_shape)\n",
    "train_shape = ((DUMMY, (*DATA_SHAPE)))\n",
    "print(\"DUMMY, (*DATA_SHAPE):\",train_shape, \"\\n\")\n",
    "\n",
    "\n",
    "train_shape = ((DATA_SIZE, *DATA_SHAPE[0]),(DATA_SIZE, *DATA_SHAPE[1]))\n",
    "test_shape = ((test_size, *DATA_SHAPE[0]),(test_size, *DATA_SHAPE[1]))\n",
    "print(train_shape, test_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_data = (np.random.sample(train_shape[0]), np.random.sample(train_shape[1]))\n",
    "test_data = (np.random.sample(test_shape[0]), np.random.sample(test_shape[1])) \n",
    "\n",
    "# used to keep track of the methodds\n",
    "log_time = {}\n",
    "\n",
    "tf.reset_default_graph()\n",
    "#sess = tf.InteractiveSession() #This needs to be closed below. Replaced it with \"with tf.Session() as sess\" in \n",
    "                                #each of the functions \n",
    "\n",
    "input_shape = [None, *DATA_SHAPE[0]] # [None, 64, 64, 3]\n",
    "output_shape = [None,*DATA_SHAPE[1]] # [None, 20]\n",
    "print(\"input_shape: {} output_shape: {}\".format(input_shape, output_shape))\n",
    "\n",
    "x, y = tf.placeholder(tf.float32, shape=input_shape), tf.placeholder(tf.float32, shape=output_shape)\n",
    "\n",
    "@how_much\n",
    "def one_shot(**kwargs):\n",
    "    print('\\none_shot:')\n",
    "    \n",
    "    #The datasets - both train and test - are loaded\n",
    "    #Two data sets. Two iterators. One for each\n",
    "    #\n",
    "    #From: https://www.tensorflow.org/api_docs/python/tf/data/Dataset#make_one_shot_iterator\n",
    "    #Note: The returned iterator will be ***initialized automatically***.\n",
    "    #A \"one-shot\" iterator ***does not*** currently support re-initialization.\n",
    "    #\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_data).batch(BATCH_SIZE).repeat()\n",
    "    train_iter = train_dataset.make_one_shot_iterator()\n",
    "    train_element = train_iter.get_next()\n",
    "\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices(test_data).batch(BATCH_SIZE).repeat()\n",
    "    test_iter = test_dataset.make_one_shot_iterator()\n",
    "    test_element = test_iter.get_next()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        for epoch_no in range(EPOCHS):\n",
    "            print(epoch_no, end=\"\")\n",
    "            for batch_no in range(N_BATCHES):\n",
    "                element = sess.run(train_element)\n",
    "                training_data_batch = element[0]\n",
    "                if(0 == epoch_no):\n",
    "                    assert(BATCH_SIZE == np.size(training_data_batch, 0))\n",
    "                else:\n",
    "                    if((epoch_no - 1) == batch_no):\n",
    "                        assert((DATA_SIZE % BATCH_SIZE) == np.size(training_data_batch, 0))\n",
    "                        #N_BATCHES (given by DATA_SIZE // BATCH_SIZE) is 78.\n",
    "                        #78 batches are read every epoch. That leaves 8 slices at the end of the epoch_no 0\n",
    "                        #Epoch 1 batch_no 0 reads only 8 when the iterator cycles back. In the remaining 77, \n",
    "                        #(77 * BATCH_SIZE) = 4928 slices would be read. Leaving (5000 - 4928) 72 slices. \n",
    "                        #So Epoch 2 will read 64 slices in batch_no 0 and then 8 slices in batch_no 1 before \n",
    "                        #the iterator goes back. So we have a batch of 8 slices rippling forward with each epoch\n",
    "                        #if (2 == epoch_no):\n",
    "                    else:\n",
    "                        assert(BATCH_SIZE == np.size(training_data_batch,0))\n",
    "\n",
    "            for _ in range(N_BATCHES):\n",
    "                sess.run(test_element)\n",
    "\n",
    "@how_much\n",
    "def initialisable(**kwargs):\n",
    "    print('\\ninitialisable:')\n",
    "    \n",
    "    #x and y are placeholders. They have to be loaded. The types and shapes are known\n",
    "    #not the values.\n",
    "    #The same placeholder is used both for train and for test data.\n",
    "    #Hence the graph needs only one dataset node and one iterator\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y)).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "    iter = dataset.make_initializable_iterator()\n",
    "    iter_init = iter.initializer\n",
    "    elements = iter.get_next()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        for i in range(EPOCHS):\n",
    "            print(i, end=\"\")\n",
    "            #Initialize to run dataset node that has been loaded with training data\n",
    "            sess.run(iter_init, feed_dict={ x: train_data[0], y: train_data[1]})\n",
    "            for _ in range(N_BATCHES):\n",
    "                sess.run(elements)\n",
    "                \n",
    "            #Re-initialize to run the ***same dataset node*** *** but with test data ***    \n",
    "            sess.run(iter_init, feed_dict={ x: test_data[0], y: test_data[1]})\n",
    "            for _ in range(N_BATCHES):\n",
    "                sess.run(elements)\n",
    "\n",
    "@how_much            \n",
    "def reinitializable(**kwargs):\n",
    "    print('\\nreinitializable:')\n",
    "    # create two datasets, one for training and one for test\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x,y)).batch(BATCH_SIZE).repeat()\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((x,y)).batch(BATCH_SIZE).repeat()\n",
    "    \n",
    "    # create ***an*** iterator of the correct shape and type\n",
    "    iter = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "    elements = iter.get_next()\n",
    "    \n",
    "    # create the nodes to initialize the iterators to iterate over the train_dataset and the test_dataset\n",
    "    # See https://www.tensorflow.org/api_docs/python/tf/data/Iterator#make_initializer.\n",
    "    #The same iterator node (iter) created above is re-initialized\n",
    "    train_init_op = iter.make_initializer(train_dataset)\n",
    "    test_init_op = iter.make_initializer(test_dataset)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        for i in range(EPOCHS):\n",
    "            print(i, end=\"\")\n",
    "            #Load the placeholders with train data and train label\n",
    "            #Iniitialize the iterator to ***iterate over training data***\n",
    "            sess.run(train_init_op, feed_dict={ x: train_data[0], y: train_data[1]})\n",
    "            for _ in range(N_BATCHES):\n",
    "                sess.run(elements)\n",
    "\n",
    "            #Load the placeholders with test data and test label\n",
    "            #RE-INITIALIZE the iter to ***now iterate over test data***\n",
    "            sess.run(test_init_op, feed_dict={ x: test_data[0], y: test_data[1]})\n",
    "            for _ in range(N_BATCHES):\n",
    "                sess.run(elements)\n",
    "\n",
    "@how_much            \n",
    "def feedable(**kwargs):\n",
    "    print('\\nfeedable:')\n",
    "    # create two datasets, one for training and one for test\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x,y)).batch(BATCH_SIZE).repeat()\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((x,y)).batch(BATCH_SIZE).repeat()\n",
    "    \n",
    "    # create the iterators from the dataset\n",
    "    train_iterator = train_dataset.make_initializable_iterator()\n",
    "    test_iterator = test_dataset.make_initializable_iterator()\n",
    "\n",
    "    handle = tf.placeholder(tf.string, shape=[])\n",
    "    \n",
    "    #See https://www.tensorflow.org/api_docs/python/tf/data/Iterator#from_string_handle\n",
    "    iter = tf.data.Iterator.from_string_handle(\n",
    "                            handle, \\\n",
    "                            train_dataset.output_types, \\\n",
    "                            train_dataset.output_shapes)\n",
    "    elements = iter.get_next()\n",
    "    \n",
    "    make_train_string_handle = train_iterator.string_handle()\n",
    "    make_test_string_handle = test_iterator.string_handle()\n",
    "    \n",
    "    init_train_iterator = train_iterator.initializer\n",
    "    init_test_iterator = test_iterator.initializer\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        #See https://www.tensorflow.org/api_docs/python/tf/data/Iterator#string_handle\n",
    "        train_string_handle = sess.run(make_train_string_handle)\n",
    "        test_string_handle = sess.run(make_test_string_handle)\n",
    "\n",
    "        #See https://www.tensorflow.org/api_docs/python/tf/data/Iterator#initializer\n",
    "        sess.run(init_train_iterator, feed_dict={ x: train_data[0], y: train_data[1]})\n",
    "        sess.run(init_test_iterator, feed_dict={ x: test_data[0], y: test_data[1]})\n",
    "\n",
    "        for i in range(EPOCHS):\n",
    "            print(i, end=\"\")\n",
    "            for _ in range(N_BATCHES):\n",
    "                sess.run(elements, feed_dict={handle: train_string_handle})\n",
    "            for _ in range(N_BATCHES):\n",
    "                sess.run(elements, feed_dict={handle: test_string_handle})\n",
    "\n",
    "print(\"\")\n",
    "one_shot(log_time=log_time)\n",
    "print(\"\")\n",
    "initialisable(log_time=log_time)\n",
    "print(\"\")\n",
    "reinitializable(log_time=log_time)\n",
    "print(\"\")\n",
    "feedable(log_time=log_time)\n",
    "\n",
    "sorted((value,key) for (key,value) in log_time.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
